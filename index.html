<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" href="style.css">
  <title>SpeechSynthesisRecorder.js - Record window.speechSynthesis.speak() with navigator.getUserMedia() and MediaRecorder</title>
</head>

<body>
<script>
  function audioBufferToWav (buffer, opt) {
    opt = opt || {}

    var numChannels = buffer.numberOfChannels
    var sampleRate = buffer.sampleRate
    var format = opt.float32 ? 3 : 1
    var bitDepth = format === 3 ? 32 : 16

    var result
    if (numChannels === 2) {
      result = interleave(buffer.getChannelData(0), buffer.getChannelData(1))
    } else {
      result = buffer.getChannelData(0)
    }

    return encodeWAV(result, format, sampleRate, numChannels, bitDepth)
  }

  function encodeWAV (samples, format, sampleRate, numChannels, bitDepth) {
    var bytesPerSample = bitDepth / 8
    var blockAlign = numChannels * bytesPerSample

    var buffer = new ArrayBuffer(44 + samples.length * bytesPerSample)
    var view = new DataView(buffer)

    /* RIFF identifier */
    writeString(view, 0, 'RIFF')
    /* RIFF chunk length */
    view.setUint32(4, 36 + samples.length * bytesPerSample, true)
    /* RIFF type */
    writeString(view, 8, 'WAVE')
    /* format chunk identifier */
    writeString(view, 12, 'fmt ')
    /* format chunk length */
    view.setUint32(16, 16, true)
    /* sample format (raw) */
    view.setUint16(20, format, true)
    /* channel count */
    view.setUint16(22, numChannels, true)
    /* sample rate */
    view.setUint32(24, sampleRate, true)
    /* byte rate (sample rate * block align) */
    view.setUint32(28, sampleRate * blockAlign, true)
    /* block align (channel count * bytes per sample) */
    view.setUint16(32, blockAlign, true)
    /* bits per sample */
    view.setUint16(34, bitDepth, true)
    /* data chunk identifier */
    writeString(view, 36, 'data')
    /* data chunk length */
    view.setUint32(40, samples.length * bytesPerSample, true)
    if (format === 1) { // Raw PCM
      floatTo16BitPCM(view, 44, samples)
    } else {
      writeFloat32(view, 44, samples)
    }

    return buffer
  }

  function interleave (inputL, inputR) {
    var length = inputL.length + inputR.length
    var result = new Float32Array(length)

    var index = 0
    var inputIndex = 0

    while (index < length) {
      result[index++] = inputL[inputIndex]
      result[index++] = inputR[inputIndex]
      inputIndex++
    }
    return result
  }

  function writeFloat32 (output, offset, input) {
    for (var i = 0; i < input.length; i++, offset += 4) {
      output.setFloat32(offset, input[i], true)
    }
  }

  function floatTo16BitPCM (output, offset, input) {
    for (var i = 0; i < input.length; i++, offset += 2) {
      var s = Math.max(-1, Math.min(1, input[i]))
      output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true)
    }
  }

  function writeString (view, offset, string) {
    for (var i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i))
    }
  }
</script>
<script>
  class SpeechSynthesisRecorder {
    constructor({
                  text = '', utteranceOptions = {}, recorderOptions = {}, dataType = ''
                }) {
      if (text === '') throw new Error('no words to synthesize')
      this.dataType = dataType
      this.text = text
      this.mimeType = MediaRecorder.isTypeSupported('audio/webm; codecs=opus') ? 'audio/webm; codecs=opus' : 'audio/ogg; codecs=opus'
      this.utterance = new SpeechSynthesisUtterance(this.text)
      this.speechSynthesis = window.speechSynthesis
      this.mediaStream_ = new MediaStream()
      this.mediaSource_ = new MediaSource()
      this.mediaRecorder = new MediaRecorder(this.mediaStream_, {
        mimeType: this.mimeType,
        bitsPerSecond: 256 * 8 * 1024
      })
      this.audioContext = new AudioContext()
      this.audioNode = new Audio()
      this.chunks = []
      if (utteranceOptions) {
        if (utteranceOptions.voice) {
          this.speechSynthesis.onvoiceschanged = e => {
            const voice = this.speechSynthesis.getVoices().find(({
                                                                   name: _name
                                                                 }) => _name === utteranceOptions.voice)
            this.utterance.voice = voice
            console.log(voice, this.utterance)
          }
          this.speechSynthesis.getVoices()
        }
        let {
          lang, rate, pitch, volume
        } = utteranceOptions;
        console.log(rate)
        Object.assign(this.utterance, {
          lang, rate, pitch, volume
        })
      }
      console.log(this.utterance)
      this.audioNode.controls = 'controls'
      document.body.appendChild(this.audioNode)
    }
    start(text = '') {
      if (text) this.text = text
      if (this.text === '') throw new Error('no words to synthesize')
      return navigator.mediaDevices.getUserMedia({
        audio: true
      })
              // set `getUserMedia()` constraints to "auidooutput", where avaialable
              // see https://bugzilla.mozilla.org/show_bug.cgi?id=934425, https://stackoverflow.com/q/33761770
              .then(stream => navigator.mediaDevices.enumerateDevices()
                      .then(devices => {
                        const audiooutput = devices.find(device => device.kind == "audiooutput");
                        stream.getTracks().forEach(track => track.stop())
                        if (audiooutput) {
                          const constraints = {
                            deviceId: {
                              exact: audiooutput.deviceId
                            }
                          };
                          return navigator.mediaDevices.getUserMedia({
                            audio: constraints
                          });
                        }
                        return navigator.mediaDevices.getUserMedia({
                          audio: true
                        });
                      }))
              .then(stream => new Promise(resolve => {
                const track = stream.getAudioTracks()[0]
                this.mediaStream_.addTrack(track)

                if (this.dataType && this.dataType === 'mediaStream') {
                  resolve({
                    tts: this,
                    data: this.mediaStream_
                  })
                };
                this.mediaRecorder.ondataavailable = event => {
                  if (event.data.size > 0) {
                    this.chunks.push(event.data)
                  };
                }
                this.mediaRecorder.onstop = () => {
                  track.stop()
                  this.mediaStream_.getAudioTracks()[0].stop()
                  this.mediaStream_.removeTrack(track)
                  console.log(`Completed recording ${this.utterance.text}`, this.chunks)
                  resolve(this)
                }
                this.mediaRecorder.start()
                this.utterance.onstart = () => {
                  console.log(`Starting recording SpeechSynthesisUtterance ${this.utterance.text}`)
                }
                this.utterance.onend = () => {
                  this.mediaRecorder.stop()
                  console.log(`Ending recording SpeechSynthesisUtterance ${this.utterance.text}`)
                }
                this.speechSynthesis.speak(this.utterance)
              }))
    }
    blob() {
      if (!this.chunks.length) throw new Error('no data to return')
      return Promise.resolve({
        tts: this,
        data: this.chunks.length === 1 ? this.chunks[0] : new Blob(this.chunks, {
          type: this.mimeType
        })
      })
    }
    arrayBuffer(blob) {
      if (!this.chunks.length) throw new Error('no data to return')
      return new Promise(resolve => {
        const reader = new FileReader()
        reader.onload = e => resolve(({
          tts: this,
          data: reader.result
        }))
        reader.readAsArrayBuffer(blob ? new Blob(blob, {
          type: blob.type
        }) : this.chunks.length === 1 ? this.chunks[0] : new Blob(this.chunks, {
          type: this.mimeType
        }))
      })
    }
    audioBuffer() {
      if (!this.chunks.length) throw new Error('no data to return')
      return this.arrayBuffer()
              .then(({
                       tts, data
                     }) => this.audioContext.decodeAudioData(data))
              .then(buffer => ({
                tts: this,
                data: buffer
              }))
    }
    mediaSource() {
      if (!this.chunks.length) throw new Error('no data to return')
      return this.arrayBuffer()
              .then(({
                       data: ab
                     }) => new Promise((resolve, reject) => {
                this.mediaSource_.onsourceended = () => resolve({
                  tts: this,
                  data: this.mediaSource_
                })
                this.mediaSource_.onsourceopen = () => {
                  if (MediaSource.isTypeSupported(this.mimeType)) {
                    const sourceBuffer = this.mediaSource_.addSourceBuffer(this.mimeType)
                    sourceBuffer.mode = 'sequence'
                    sourceBuffer.onupdateend = () =>
                            this.mediaSource_.endOfStream()
                    sourceBuffer.appendBuffer(ab)
                  } else {
                    reject(new Error(`${this.mimeType} is not supported`))
                  }
                }
                this.audioNode.src = URL.createObjectURL(this.mediaSource_)
              }))
    }
    readableStream({
                     size = 1024, controllerOptions = {}, rsOptions = {}
                   }) {
      if (!this.chunks.length) throw new Error('no data to return')
      const src = this.chunks.slice(0)
      const chunk = size
      return Promise.resolve({
        tts: this,
        data: new ReadableStream(controllerOptions || {
          start(controller) {
            console.log(src.length)
            controller.enqueue(src.splice(0, chunk))
          },
          pull(controller) {
            if (src.length === 0) controller.close()
            controller.enqueue(src.splice(0, chunk))
          }
        }, rsOptions)
      })
    }
  }
 function test() {
   const data = document.querySelector('textarea');
   let ttsRecorder = new SpeechSynthesisRecorder({
     text: data.value,
     utteranceOptions: {
       voice: "english-us espeak",
       lang: "en-US",
       pitch: .75,
       volume: 1,
       rate: 1
     }
   });

   ttsRecorder.start()
           .then(tts => tts.blob())
           .then(({
                    tts, data
                  }) => {
             // do stuff with `ArrayBuffer`, `AudioBuffer`, `Blob`, `MediaSource`, `MediaStream`, `ReadableStream`

             tts.audioBuffer().then(({ data }) => {
               var anchor = document.createElement('a')
               document.body.appendChild(anchor)
               anchor.style = 'display: none'
               const wav = audioBufferToWav(data)

               const blob = new window.Blob([new DataView(wav)], {
                 type: 'audio/wav'
               })

               var url = window.URL.createObjectURL(blob)
               anchor.href = url
               anchor.download = `test.wav`
               anchor.click()
               window.URL.revokeObjectURL(url)
             })

           })
           .catch(err => console.log(err))
 }
</script>
<div>
  <textarea></textarea>
  <button onclick="test()">Listen & Create file</button>
</div>
</body>

</html>
